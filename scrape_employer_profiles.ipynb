{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFQIIfKPjve4"
      },
      "source": [
        "# Scraping all Kununu Websites for German Firms\n",
        "\n",
        "Information to scrape:\n",
        "\n",
        "- Firm name\n",
        "- Firm uuid\n",
        "- Number of views\n",
        "- Overall rating\n",
        "- Percentage of people who would recommend the firm\n",
        "- Total number of reviews\n",
        "- Number of salaries posted\n",
        "- Number of corporate culture reviews\n",
        "- Ratings for each category\n",
        "- Number of reviews and scores by applicants\n",
        "- Number of reviews and scores by employees\n",
        "\n",
        "These require scraping three different pages:\n",
        "- Main Page: https://www.kununu.com/de/[company name] \n",
        "- Total Views: https://www.kununu.com/middlewares/profiles/+[company uuid]+/statistics \n",
        "- Applicant Reviews: https://www.kununu.com/de/[company name]/bewerbung\n",
        "- Employee Reviews: https://www.kununu.com/de/[company name]/kommentare\n",
        "\n",
        "**Important Note**: This code works as of July 12th, 2024. Kununu may change their website structure, which would require updating the code.\n",
        "In particular, it is likely that the CLASS_IDS dictionary will need to be updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install requests beautifulsoup4 pandas numpy python-dotenv\n",
        "\n",
        "from urllib.parse import urlencode\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils import *\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# load_dotenv() # make sure to have a .env file that defines the variable 'SCRAPINGBEE_API_KEY' if using scrapingbee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONCURRENCY = 10 # Number of concurrent requests to make when scraping\n",
        "CLASS_IDS = {\n",
        "    \"firm_name\": \"index__title__0q4vx h3-semibold\",\n",
        "    \"percent_recommend\": \"index__value__o0UJI h2 h3-semibold-tablet\",\n",
        "    \"overall_rating\": \"index__value__ApL+4 h2 h3-semibold-tablet\",\n",
        "    \"tabs\": \"index__tabs__lGVpv\",\n",
        "    \"factor_score\": \"^index__factorScore\",\n",
        "    \"total_reviews\": \"index__totalReviews__aUzS6 p-small-semibold\",\n",
        "    \"aggregation\": \"index__aggregation__NhXCC index__center__K0n3a\",\n",
        "    \"employee_score\": \"h3-semibold index__score__BktQY\",\n",
        "    \"employee_recommendation\": \"index__recommendation__LS0nx\"\n",
        "}\n",
        "\n",
        "# importing all Kununu links\n",
        "pwd = os.getcwd()\n",
        "with open(pwd+\"/data/all_kununu_company_profile_links.txt\", \"r\") as file:\n",
        "    FileContent = file.read()\n",
        "all_kununu_links = FileContent.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_yIg22RMZpWr"
      },
      "outputs": [],
      "source": [
        "def main_page_scrape(url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Takes in a URL and returns a dictionary with all the information scraped from the URL to ratings_list.\n",
        "    Information collected:\n",
        "    - Firm name\n",
        "    - Firm uuid\n",
        "    - Number of views\n",
        "    - Overall rating\n",
        "    - Percentage of people who would recommend the firm\n",
        "    - Total number of reviews\n",
        "    - Number of salaries posted\n",
        "    - Number of corporate culture reviews\n",
        "    - Ratings for each category\n",
        "    \"\"\"\n",
        "    result_dict = {}\n",
        "    soup = soup_from_url(url)\n",
        "\n",
        "    result_dict[\"firm_name\"] = soup.find(class_=CLASS_IDS[\"firm_name\"]).text.replace(\",\", \".\").replace('\\xa0', ' ') if soup.find(class_=CLASS_IDS[\"firm_name\"]) else None\n",
        "    result_dict[\"url\"] = url\n",
        "    result_dict[\"uuid\"] = str(soup).split('\"uuid\":\"')[1].split('\"')[0] if '\"uuid\":\"' in str(soup) else None\n",
        "\n",
        "    try:\n",
        "        num_views = requests.get(f\"https://www.kununu.com/middlewares/profiles/{result_dict['uuid']}/statistics\").text\n",
        "        result_dict[\"views_num\"] = int(num_views.split('\"totalViews\":')[1].split(',')[0])\n",
        "    except:\n",
        "        result_dict[\"views_num\"] = np.nan\n",
        "\n",
        "    result_dict[\"percent_recommend_overall\"] = int(soup.find(class_=CLASS_IDS[\"percent_recommend\"]).text.replace(\".\", \"\").replace(\",\", \"\").replace(\"%\", \"\")) if soup.find(class_=CLASS_IDS[\"percent_recommend\"]) else np.nan\n",
        "    result_dict[\"overall_rating\"] = float(soup.find(class_=CLASS_IDS[\"overall_rating\"]).text.replace(\",\", \".\")) if soup.find(class_=CLASS_IDS[\"overall_rating\"]) else np.nan\n",
        "\n",
        "    num_revs_soup = re.findall(r'\\(.*?\\)', str(soup.find(class_=CLASS_IDS[\"tabs\"]).text)) if soup.find(class_=CLASS_IDS[\"tabs\"]) else None\n",
        "    if num_revs_soup:\n",
        "        num_revs_ls = [int(x.replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\")) for x in num_revs_soup]\n",
        "        result_dict[\"total_reviews_num\"] = num_revs_ls[0]\n",
        "        result_dict[\"salaries_posted_num\"] = num_revs_ls[1] if len(num_revs_ls) > 1 else np.nan\n",
        "        result_dict[\"corporate_culture_review_num\"] = num_revs_ls[2] if len(num_revs_ls) > 2 else np.nan\n",
        "    else:\n",
        "        result_dict[\"total_reviews_num\"], result_dict[\"salaries_posted_num\"], result_dict[\"corporate_culture_review_num\"] = np.nan, np.nan, np.nan\n",
        "\n",
        "    ratings_raw = [x.parent.text for x in soup.find_all(class_=re.compile(CLASS_IDS[\"factor_score\"]))]\n",
        "    categories = [rating[3:] for rating in ratings_raw]\n",
        "    ratings = [float(str(rating[:3].replace(\",\", \".\"))) for rating in ratings_raw]\n",
        "    result_dict.update(dict(zip(categories, ratings)))\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "def get_applicant_info(url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Takes in a url and returns the review scores and number of reviews by applicants to the company,\n",
        "    separated by the following categories: \"hired\", \"rejected\", \"offerDeclined\", \"deferred\".\n",
        "    \"\"\"\n",
        "    application_outcomes = [\"hired\", \"rejected\", \"offerDeclined\", \"deferred\"]\n",
        "    reviews_by_applicants = {}\n",
        "\n",
        "    for outcome in [\"all_applicants\"] + application_outcomes:\n",
        "        soup = soup_from_url(f\"{url}/bewerbung{'?result=' + outcome if outcome != 'all_applicants' else ''}\")\n",
        "        try:\n",
        "            reviews_by_applicants[f\"{outcome}_review_num\"] = int(soup.find(class_=CLASS_IDS[\"total_reviews\"]).text.split(\" \")[0])\n",
        "            reviews_by_applicants[f\"{outcome}_review_score\"] = float(soup.find(class_=CLASS_IDS[\"aggregation\"]).text[:3].replace(\",\", \".\"))\n",
        "        except:\n",
        "            reviews_by_applicants[f\"{outcome}_review_num\"] = np.nan\n",
        "            reviews_by_applicants[f\"{outcome}_review_score\"] = np.nan\n",
        "\n",
        "    return reviews_by_applicants\n",
        "\n",
        "def get_employee_info(url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Takes in a url and returns the review scores, number of reviews by employees to the company,\n",
        "    and percent of employees that would recommend the company. \n",
        "    \"\"\"\n",
        "    reviews_by_employees = {}\n",
        "    soup = soup_from_url(f\"{url}/kommentare\")\n",
        "\n",
        "    try:\n",
        "        reviews_by_employees[\"employees_review_num\"] = int(soup.find(class_=CLASS_IDS[\"total_reviews\"]).text.split(\" \")[0].replace(\".\", \"\"))\n",
        "        reviews_by_employees[\"employee_review_score\"] = float(soup.find(class_=CLASS_IDS[\"employee_score\"]).text[:3].replace(\",\", \".\"))\n",
        "        reviews_by_employees[\"employee_rec_score\"] = int(soup.find(class_=CLASS_IDS[\"employee_recommendation\"]).text.split(\"%\")[0])\n",
        "    except:\n",
        "        reviews_by_employees[\"employees_review_num\"] = np.nan\n",
        "        reviews_by_employees[\"employee_review_score\"] = np.nan\n",
        "        reviews_by_employees[\"employee_rec_score\"] = np.nan\n",
        "\n",
        "    return reviews_by_employees\n",
        "\n",
        "def get_all_info(url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Takes in a url and returns all the information scraped from the URL to ratings_list.\n",
        "    Information collected:\n",
        "    - Firm name\n",
        "    - Firm uuid\n",
        "    - Number of views\n",
        "    - Overall rating\n",
        "    - Percentage of people who would recommend the firm\n",
        "    - Total number of reviews\n",
        "    - Number of salaries posted\n",
        "    - Number of corporate culture reviews\n",
        "    - Ratings for each category\n",
        "    - Number of reviews and scores by applicants\n",
        "    - Number of reviews and scores by employees\n",
        "    \"\"\"\n",
        "    result_dict = main_page_scrape(url)\n",
        "    result_dict.update(get_applicant_info(url))\n",
        "    result_dict.update(get_employee_info(url))\n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_name_mapping = {\n",
        "    'firm_name': 'firm_name',\n",
        "    'url': 'kn_url',\n",
        "    'uuid': 'uuid',\n",
        "    'views_num': 'kn_views_num',\n",
        "    'percent_recommend_overall': 'kn_employee_rec_score',\n",
        "    'overall_rating': 'kn_overall',\n",
        "    'total_reviews_num': 'kn_total_reviews_num',\n",
        "    'salaries_posted_num': 'kn_salaries_posted_num',\n",
        "    'gehaltsozialleistungen': 'kn_salary_benefits',\n",
        "    'image': 'kn_image',\n",
        "    'karriereweiterbildung': 'kn_career_development',\n",
        "    'arbeitsatmosphare': 'kn_work_atmosphere',\n",
        "    'kommunikation': 'kn_communication',\n",
        "    'kollegenzusammenhalt': 'kn_colleague_cohesion',\n",
        "    'work_life_balance': 'kn_work_life_balance',\n",
        "    'vorgesetztenverhalten': 'kn_superiors_behavior',\n",
        "    'interessante aufgaben': 'kn_interesting_tasks',\n",
        "    'arbeitsbedingungen': 'kn_working_conditions',\n",
        "    'umwelt_sozialbewusstsein': 'kn_environment_social_awareness',\n",
        "    'gleichberechtigung': 'kn_equal_rights',\n",
        "    'umgang mit alteren kollegen': 'kn_dealing_with_older_colleagues',\n",
        "    'all_applicants_review_num': 'kn_all_applicants_review_num',\n",
        "    'all_applicants_review_score': 'kn_all_applicants_review_score',\n",
        "    'hired_review_num': 'kn_hired_review_num',\n",
        "    'hired_review_score': 'kn_hired_score',\n",
        "    'rejected_review_num': 'kn_rejected_review_num',\n",
        "    'rejected_review_score': 'kn_rejected_score',\n",
        "    'offerdeclined_review_num': 'kn_offer_declined_review_num',\n",
        "    'offerdeclined_review_score': 'kn_offer_declined_score',\n",
        "    'deferred_review_num': 'kn_deferred_review_num',\n",
        "    'deferred_review_score': 'kn_deferred_score',\n",
        "    'employees_review_num': 'kn_employees_review_num',\n",
        "    'employee_review_score': 'kn_employee_review_score',\n",
        "    'employee_rec_score': 'kn_employee_rec_score',\n",
        "    'corporate_culture_review_num': 'kn_corporate_culture_review_num'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scraping all data in parallel and saving to csv\n",
        "window_size = 5000\n",
        "for i in range(0, len(all_kununu_links)//window_size+1):\n",
        "    concurrency = CONCURRENCY\n",
        "    pool = ThreadPool(concurrency)\n",
        "    ratings_list = pool.map(get_all_info, all_kununu_links[i*window_size:min((i+1)*window_size, len(all_kununu_links))])\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    df = pd.DataFrame(ratings_list)\n",
        "    df.columns = [s.replace(\"/\", \"s\").replace(\"-\",\"_\").replace(\"Ã¤\", \"a\").lower() for s in df.columns]\n",
        "    df.rename(columns=column_name_mapping, inplace=True)\n",
        "    df.to_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\", index=False)\n",
        "    print(f\"Saved results to data/kununu_data_{i+1}.csv\")\n",
        "\n",
        "# Consolidate all results\n",
        "df = pd.concat([pd.read_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\") for i in range(0, len(all_kununu_links)//window_size+1)], ignore_index=True)\n",
        "df.to_csv(f\"{pwd}/data/all_scraped_kununu_data.csv\", index=False)\n",
        "print(\"Saved results to data/all_scraped_kununu_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vlh9acpqLAmT"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
