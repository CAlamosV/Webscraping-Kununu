{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFQIIfKPjve4"
   },
   "source": [
    "# Scraping all Kununu Websites for German Firms\n",
    "\n",
    "Information to scrape:\n",
    "\n",
    "- Firm name\n",
    "- Firm uuid\n",
    "- Number of views\n",
    "- Overall rating\n",
    "- Percentage of people who would recommend the firm\n",
    "- Total number of reviews\n",
    "- Number of salaries posted\n",
    "- Number of corporate culture reviews\n",
    "- Ratings for each category\n",
    "- Number of reviews and scores by applicants\n",
    "- Number of reviews and scores by employees\n",
    "\n",
    "These require scraping three different pages:\n",
    "- Main Page: https://www.kununu.com/de/[company name] \n",
    "- Total Views: https://www.kununu.com/middlewares/profiles/+[company uuid]+/statistics \n",
    "- Applicant Reviews: https://www.kununu.com/de/[company name]/bewerbung\n",
    "- Employee Reviews: https://www.kununu.com/de/[company name]/kommentare\n",
    "\n",
    "**Important Note**: This code works as of December 10th, 2024. Kununu may change their website structure, which would require updating the code.\n",
    "In particular, it is likely that the CLASS_IDS dictionary will need to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install requests beautifulsoup4 pandas numpy python-dotenv\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv() # make sure to have a .env file that defines the variable 'SCRAPINGBEE_API_KEY' if using scrapingbee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY = 10 # Number of concurrent requests to make when scraping\n",
    "CLASS_IDS = {\n",
    "    \"firm_name\": \"index__title__0q4vx h3-semibold\",\n",
    "    \"percent_recommend\": \"index__value__o0UJI h2 h3-semibold-tablet\",\n",
    "    \"overall_rating\": \"index__value__ApL+4 h2 h3-semibold-tablet\",\n",
    "    \"tabs\": \"index__tabs__lGVpv\",\n",
    "    \"factor_score\": \"^index__factorScore\",\n",
    "    \"total_reviews\": \"index__totalReviews__aUzS6 p-small-semibold\",\n",
    "    \"aggregation\": \"index__aggregation__NhXCC index__center__K0n3a\",\n",
    "    \"employee_score\": \"h3-semibold index__score__BktQY\",\n",
    "    \"employee_recommendation\": \"index__recommendation__LS0nx\",\n",
    "    \"benefits\": \"index__benefitsContent__7VqX6\",\n",
    "    \"benefit_items\": \"index__benefitItem__R7P9I\",\n",
    "    \"benefit_title\": \"index__benefitTitle__zmARM\",\n",
    "    \"benefit_percentage\": \"index__benefitPercentage__Z1tVV\",\n",
    "    \"satisfied_salary\": \"index__rate__N7w1O\",\n",
    "}\n",
    "\n",
    "# importing all Kununu links\n",
    "pwd = os.getcwd()\n",
    "with open(pwd+\"/data/all_kununu_company_profile_links.txt\", \"r\") as file:\n",
    "    FileContent = file.read()\n",
    "all_kununu_links = FileContent.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yIg22RMZpWr"
   },
   "outputs": [],
   "source": [
    "def get_stats_page(uuid: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"Scrape stats page, return fully flattened dictionary with unstacked recommendation_rate.\"\"\"\n",
    "    response = requests.get(f\"https://www.kununu.com/middlewares/profiles/{uuid}\")\n",
    "    data_dict = replace_null_with_none(response.text)\n",
    "    data_dict = flatten(convert_keys_to_snake_case(data_dict))\n",
    "\n",
    "    # Manually un-nest recommendation_rate\n",
    "    recommendation_rate = data_dict.get(\"reviews\", {}).get(\"recommendation_rate\", {})\n",
    "    recommendation_rate_unstacked = {\n",
    "        \"recommendation_rate_percentage\": recommendation_rate.get(\"percentage\"),\n",
    "        \"recommendation_rate_total_reviews\": recommendation_rate.get(\"total_reviews\"),\n",
    "        \"recommendation_rate_recommended_total_reviews\": recommendation_rate.get(\"recommended_total_reviews\"),\n",
    "        \"recommendation_rate_not_recommended_total_reviews\": recommendation_rate.get(\"not_recommended_total_reviews\"),\n",
    "    }\n",
    "    data_dict.update(recommendation_rate_unstacked)\n",
    "    data_dict.pop(\"reviews_recommendation_rate\", None)\n",
    "\n",
    "    data_dict = {key: data_dict.get(key) for key in required_keys}\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def main_page_scrape(url: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a URL and returns a dictionary with all the information scraped from the URL to ratings_list.\n",
    "    Information collected:\n",
    "    - Firm name\n",
    "    - Firm uuid\n",
    "    - Number of views\n",
    "    - Overall rating\n",
    "    - Percentage of people who would recommend the firm\n",
    "    - Total number of reviews\n",
    "    - Number of salaries posted\n",
    "    - Number of corporate culture reviews\n",
    "    - Ratings for each category\n",
    "    - Benefits\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    soup = soup_from_url(url)\n",
    "\n",
    "    result_dict[\"url\"] = url\n",
    "    result_dict[\"uuid\"] = str(soup).split('\"uuid\":\"')[1].split('\"')[0] if '\"uuid\":\"' in str(soup) else None\n",
    "\n",
    "    num_revs_soup = re.findall(r'\\(.*?\\)', str(soup.find(class_=CLASS_IDS[\"tabs\"]).text)) if soup.find(class_=CLASS_IDS[\"tabs\"]) else None\n",
    "    if num_revs_soup:\n",
    "        num_revs_ls = [int(x.replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\")) for x in num_revs_soup]\n",
    "        result_dict[\"salaries_posted_num\"] = num_revs_ls[1] if len(num_revs_ls) > 1 else np.nan\n",
    "        result_dict[\"corporate_culture_review_num\"] = num_revs_ls[2] if len(num_revs_ls) > 2 else np.nan\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Failed to retrieve number of reviews, salaries posted, or corporate culture reviews.\")\n",
    "        result_dict[\"total_reviews_num\"], result_dict[\"salaries_posted_num\"], result_dict[\"corporate_culture_review_num\"] = np.nan, np.nan, np.nan\n",
    "    \n",
    "    if verbose:\n",
    "        try:    \n",
    "            # Retrieve Satisfied Salary Percentage\n",
    "            satisfied_salary_section = soup.find(class_=CLASS_IDS[\"satisfied_salary\"])\n",
    "            \n",
    "            # Extract the percentage value, if available\n",
    "            if satisfied_salary_section:\n",
    "                result_dict[\"satisfied_salary_pct\"] = int(\n",
    "                    satisfied_salary_section.text.split()[0].strip().replace(\"%\", \"\")\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Satisfied salary section not found in the HTML. Ensure that the page structure has not changed.\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Failed to retrieve satisfied salary percentage: {e}\")\n",
    "            result_dict[\"satisfied_salary_pct\"] = np.nan\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        try:\n",
    "            benefits_section = soup.find(class_=CLASS_IDS[\"benefits\"])\n",
    "            \n",
    "            if not benefits_section:\n",
    "                raise ValueError(\"Benefits section not found in the HTML. Ensure that the page structure has not changed.\")\n",
    "            \n",
    "            benefit_items = benefits_section.find_all(\"li\", class_=CLASS_IDS[\"benefit_items\"])\n",
    "            \n",
    "            if not benefit_items:\n",
    "                raise ValueError(\"No benefit items found in the benefits section. Verify if benefits are listed properly or the HTML class has been updated.\")\n",
    "            \n",
    "            for benefit_item in benefit_items:\n",
    "                try:\n",
    "                    benefit_name = benefit_item.find(\"span\", class_=CLASS_IDS[\"benefit_title\"]).text.strip()\n",
    "                    benefit_percentage = benefit_item.find(\"div\", class_=CLASS_IDS[\"benefit_percentage\"]).find(\"span\", class_=\"legend-regular\").text.strip().replace(\"%\", \"\")\n",
    "                    result_dict[benefit_name] = int(benefit_percentage)\n",
    "                except AttributeError as inner_e:\n",
    "                    if verbose:\n",
    "                        print(f\"Failed to extract name or percentage for a benefit item: {inner_e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Failed to retrieve benefits data: {e}\")\n",
    "    else:\n",
    "        benefits_section = soup.find(class_=CLASS_IDS[\"benefits\"])\n",
    "        if benefits_section:\n",
    "            benefit_items = benefits_section.find_all(\"li\", class_=CLASS_IDS[\"benefit_items\"])\n",
    "            for benefit_item in benefit_items:\n",
    "                try:\n",
    "                    benefit_name = benefit_item.find(\"span\", class_=CLASS_IDS[\"benefit_title\"]).text.strip()\n",
    "                    benefit_percentage = benefit_item.find(\"div\", class_=CLASS_IDS[\"benefit_percentage\"]).find(\"span\", class_=\"legend-regular\").text.strip().replace(\"%\", \"\")\n",
    "                    result_dict[benefit_name] = int(benefit_percentage)\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "\n",
    "    ratings_raw = [x.parent.text for x in soup.find_all(class_=re.compile(CLASS_IDS[\"factor_score\"]))]\n",
    "    categories = [rating[3:] for rating in ratings_raw]\n",
    "    ratings = [float(str(rating[:3].replace(\",\", \".\"))) for rating in ratings_raw]\n",
    "    result_dict.update(dict(zip(categories, ratings)))\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def get_applicant_info(url: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns the review scores and number of reviews by applicants to the company,\n",
    "    separated by the following categories: \"hired\", \"rejected\", \"offerDeclined\", \"deferred\".\n",
    "    \"\"\"\n",
    "    application_outcomes = [\"hired\", \"rejected\", \"offerDeclined\", \"deferred\"]\n",
    "    reviews_by_applicants = {}\n",
    "\n",
    "    for outcome in [\"all_applicants\"] + application_outcomes:\n",
    "        soup = soup_from_url(f\"{url}/bewerbung{'?result=' + outcome if outcome != 'all_applicants' else ''}\")\n",
    "        try:\n",
    "            reviews_by_applicants[f\"{outcome}_review_num\"] = int(soup.find(class_=CLASS_IDS[\"total_reviews\"]).text.split(\" \")[0])\n",
    "            reviews_by_applicants[f\"{outcome}_review_score\"] = float(soup.find(class_=CLASS_IDS[\"aggregation\"]).text[:3].replace(\",\", \".\"))\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Failed to retrieve review number or score for outcome '{outcome}': {e}\")\n",
    "            reviews_by_applicants[f\"{outcome}_review_num\"] = np.nan\n",
    "            reviews_by_applicants[f\"{outcome}_review_score\"] = np.nan\n",
    "\n",
    "    return reviews_by_applicants\n",
    "\n",
    "def get_employee_info(url: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns the review scores, number of reviews by employees to the company,\n",
    "    and percent of employees that would recommend the company.\n",
    "    \"\"\"\n",
    "    reviews_by_employees = {\n",
    "        \"sehr_gut_reviews\": np.nan,\n",
    "        \"gut_reviews\": np.nan,\n",
    "        \"befriedigend_reviews\": np.nan,\n",
    "        \"genuegend_reviews\": np.nan,\n",
    "    }\n",
    "    soup = soup_from_url(f\"{url}/kommentare\")\n",
    "\n",
    "    try:\n",
    "        reviews_by_employees[\"employees_review_num\"] = int(soup.find(class_=CLASS_IDS[\"total_reviews\"]).text.split(\" \")[0].replace(\".\", \"\"))\n",
    "        reviews_by_employees[\"employee_review_score\"] = float(soup.find(class_=CLASS_IDS[\"employee_score\"]).text[:3].replace(\",\", \".\"))\n",
    "        reviews_by_employees[\"employee_rec_score\"] = int(soup.find(class_=CLASS_IDS[\"employee_recommendation\"]).text.split(\"%\")[0])\n",
    "\n",
    "        # Adding employee review scores for each category\n",
    "        category_buttons = soup.find_all(\"button\", {\"aria-label\": \"Reviews score detail\"})\n",
    "        for button in category_buttons:\n",
    "            category_name_german = button.find(\"span\", class_=\"index__category__fvg57\").text.strip()\n",
    "            review_count_text = button.find(\"span\", class_=\"index__totalReviews__6pCSR\").text.split(\" \")[0]\n",
    "            review_count = int(review_count_text.replace(\".\", \"\").replace(\",\", \"\"))\n",
    "\n",
    "            # Match against German category names\n",
    "            if category_name_german == \"Sehr gut\":\n",
    "                reviews_by_employees[\"sehr_gut_reviews\"] = review_count\n",
    "            elif category_name_german == \"Gut\":\n",
    "                reviews_by_employees[\"gut_reviews\"] = review_count\n",
    "            elif category_name_german == \"Befriedigend\":\n",
    "                reviews_by_employees[\"befriedigend_reviews\"] = review_count\n",
    "            elif category_name_german == \"Genügend\":\n",
    "                reviews_by_employees[\"genuegend_reviews\"] = review_count\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Failed to retrieve employee review information: {e}\")\n",
    "        reviews_by_employees[\"employees_review_num\"] = np.nan\n",
    "        reviews_by_employees[\"employee_review_score\"] = np.nan\n",
    "        reviews_by_employees[\"employee_rec_score\"] = np.nan\n",
    "\n",
    "    return reviews_by_employees\n",
    "\n",
    "def get_all_info(url: str, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns all the information scraped from the URL to a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    result_dict = main_page_scrape(url, verbose=verbose)\n",
    "    result_dict.update(get_applicant_info(url, verbose=verbose))\n",
    "    result_dict.update(get_employee_info(url, verbose=verbose))\n",
    "    result_dict.update(get_stats_page(result_dict[\"uuid\"]))\n",
    "    df = pd.DataFrame([result_dict])\n",
    "    df.columns = [x.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"\").replace(\"ä\", \"a\").lower() for x in df.columns]\n",
    "    df = df.rename(columns=column_name_mapping)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['kn_url', 'uuid', 'kn_salaries_posted_num',\n",
       "       'kn_corporate_culture_review_num', 'kn_satisfied_salary_pct',\n",
       "       'kn_canteen', 'kn_flexible_working_hours', 'kn_company_doctor',\n",
       "       'kn_company_retirement_provision', 'kn_parking', 'kn_home_office',\n",
       "       'kn_health_measures', 'kn_discounts', 'kn_company_phone',\n",
       "       'kn_employee_participation', 'kn_internet_usage',\n",
       "       'kn_good_transport_links', 'kn_employee_events', 'kn_food_allowance',\n",
       "       'kn_accessible', 'kn_coaching', 'kn_childcare', 'kn_company_car',\n",
       "       'kn_dogs_allowed', 'kn_salary_benefits', 'kn_image',\n",
       "       'kn_career_development', 'kn_work_atmosphere', 'kn_communication',\n",
       "       'kn_colleague_cohesion', 'kn_work_life_balance',\n",
       "       'kn_superiors_behavior', 'kn_interesting_tasks',\n",
       "       'kn_working_conditions', 'kn_environment_social_awareness',\n",
       "       'kn_equal_rights', 'kn_dealing_with_older_colleagues',\n",
       "       'kn_all_applicants_review_num', 'kn_all_applicants_review_score',\n",
       "       'kn_hired_review_num', 'kn_hired_score', 'kn_rejected_review_num',\n",
       "       'kn_rejected_score', 'kn_offer_declined_review_num',\n",
       "       'kn_offer_declined_score', 'kn_deferred_review_num',\n",
       "       'kn_deferred_score', 'kn_very_good_reviews', 'kn_good_reviews',\n",
       "       'kn_satisfactory_reviews', 'kn_insufficient_reviews',\n",
       "       'kn_employees_review_num', 'kn_employee_review_score',\n",
       "       'kn_employee_rec_score', 'kn_firm_name', 'kn_firm_simple_name',\n",
       "       'kn_canonical_slug', 'kn_country_code', 'kn_city', 'kn_state',\n",
       "       'kn_locations_total', 'kn_is_verified', 'kn_is_claimed',\n",
       "       'kn_is_top_company_paid', 'kn_industry_id', 'kn_total_companies',\n",
       "       'kn_reviews_published_last_two_years',\n",
       "       'kn_reviews_offline_deleted_last_two_years', 'kn_reviews_first_year',\n",
       "       'kn_industry_avg_score', 'kn_recommendation_rate_pct',\n",
       "       'kn_recommendation_total_reviews',\n",
       "       'kn_recommendation_total_positive_reviews',\n",
       "       'kn_recommendation_total_negative_reviews'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_info('https://www.kununu.com/de/siemens').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to data/kununu_data_1.csv\n",
      "Saved results to data/kununu_data_2.csv\n",
      "Saved results to data/kununu_data_3.csv\n",
      "Saved results to data/kununu_data_4.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m concurrency \u001b[38;5;241m=\u001b[39m CONCURRENCY\n\u001b[1;32m      5\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(concurrency)\n\u001b[0;32m----> 6\u001b[0m ratings_list \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_all_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_kununu_links\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_kununu_links\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      8\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:634\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 634\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:334\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Scraping all data in parallel and saving to csv\n",
    "window_size = 5\n",
    "for i in range(0, 10):#len(all_kununu_links)//window_size+1):\n",
    "    concurrency = CONCURRENCY\n",
    "    pool = ThreadPool(concurrency)\n",
    "    df = pool.map(get_all_info, all_kununu_links[i*window_size:min((i+1)*window_size, len(all_kununu_links))])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    df.to_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\", index=False)\n",
    "    print(f\"Saved results to data/kununu_data_{i+1}.csv\")\n",
    "\n",
    "# Consolidate all results\n",
    "df = pd.concat([pd.read_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\") for i in range(0, len(all_kununu_links)//window_size+1)], ignore_index=True)\n",
    "df.to_csv(f\"{pwd}/data/all_scraped_kununu_data.csv\", index=False)\n",
    "print(\"Saved results to data/all_scraped_kununu_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vlh9acpqLAmT"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
