{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFQIIfKPjve4"
   },
   "source": [
    "# Scraping all Kununu Websites for German Firms\n",
    "\n",
    "Information to scrape:\n",
    "\n",
    "- Firm name\n",
    "- Firm uuid\n",
    "- Number of views\n",
    "- Overall rating\n",
    "- Percentage of people who would recommend the firm\n",
    "- Total number of reviews\n",
    "- Number of salaries posted\n",
    "- Number of corporate culture reviews\n",
    "- Ratings for each category\n",
    "- Number of reviews and scores by applicants\n",
    "- Number of reviews and scores by employees\n",
    "\n",
    "These require scraping three different pages:\n",
    "- Main Page: https://www.kununu.com/de/[company name] \n",
    "- Total Views: https://www.kununu.com/middlewares/profiles/+[company uuid]+/statistics \n",
    "- Applicant Reviews: https://www.kununu.com/de/[company name]/bewerbung\n",
    "- Employee Reviews: https://www.kununu.com/de/[company name]/kommentare\n",
    "\n",
    "**Important Note**: This code works as of July 12th, 2024. Kununu may change their website structure, which would require updating the code.\n",
    "In particular, it is likely that the CLASS_IDS dictionary will need to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install requests beautifulsoup4 pandas numpy python-dotenv\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # make sure to have a .env file that defines the variable 'SCRAPINGBEE_API_KEY' if using scrapingbee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY = 10 # Number of concurrent requests to make when scraping\n",
    "CLASS_IDS = {\n",
    "    \"firm_name\": \"index__title__0q4vx h3-semibold\",\n",
    "    \"percent_recommend\": \"index__value__o0UJI h2 h3-semibold-tablet\",\n",
    "    \"overall_rating\": \"index__value__ApL+4 h2 h3-semibold-tablet\",\n",
    "    \"tabs\": \"index__tabs__lGVpv\",\n",
    "    \"factor_score\": \"^index__factorScore\",\n",
    "    \"total_reviews\": \"index__totalReviews__aUzS6 p-small-semibold\",\n",
    "    \"aggregation\": \"index__aggregation__NhXCC index__center__K0n3a\",\n",
    "    \"employee_score\": \"h3-semibold index__score__BktQY\",\n",
    "    \"employee_recommendation\": \"index__recommendation__LS0nx\"\n",
    "}\n",
    "\n",
    "# importing all Kununu links\n",
    "pwd = os.getcwd()\n",
    "with open(pwd+\"/data/all_kununu_company_profile_links.txt\", \"r\") as file:\n",
    "    FileContent = file.read()\n",
    "all_kununu_links = FileContent.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_yIg22RMZpWr"
   },
   "outputs": [],
   "source": [
    "def main_page_scrape(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a URL and returns a dictionary with all the information scraped from the URL to ratings_list.\n",
    "    Information collected:\n",
    "    - Firm name\n",
    "    - Firm uuid\n",
    "    - Number of views\n",
    "    - Overall rating\n",
    "    - Percentage of people who would recommend the firm\n",
    "    - Total number of reviews\n",
    "    - Number of salaries posted\n",
    "    - Number of corporate culture reviews\n",
    "    - Ratings for each category\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    soup = soup_from_url(url)\n",
    "\n",
    "    result_dict[\"firm_name\"] = soup.find(class_=CLASS_IDS[\"firm_name\"]).text.replace(\",\", \".\").replace('\\xa0', ' ') if soup.find(class_=CLASS_IDS[\"firm_name\"]) else None\n",
    "    result_dict[\"url\"] = url\n",
    "    result_dict[\"uuid\"] = str(soup).split('\"uuid\":\"')[1].split('\"')[0] if '\"uuid\":\"' in str(soup) else None\n",
    "\n",
    "    try:\n",
    "        num_views = requests.get(f\"https://www.kununu.com/middlewares/profiles/{result_dict['uuid']}/statistics\").text\n",
    "        result_dict[\"views_num\"] = int(num_views.split('\"totalViews\":')[1].split(',')[0])\n",
    "    except:\n",
    "        result_dict[\"views_num\"] = np.nan\n",
    "\n",
    "    result_dict[\"percent_recommend_overall\"] = int(soup.find(class_=CLASS_IDS[\"percent_recommend\"]).text.replace(\".\", \"\").replace(\",\", \"\").replace(\"%\", \"\")) if soup.find(class_=CLASS_IDS[\"percent_recommend\"]) else np.nan\n",
    "    result_dict[\"overall_rating\"] = float(soup.find(class_=CLASS_IDS[\"overall_rating\"]).text.replace(\",\", \".\")) if soup.find(class_=CLASS_IDS[\"overall_rating\"]) else np.nan\n",
    "\n",
    "    num_revs_soup = re.findall(r'\\(.*?\\)', str(soup.find(class_=CLASS_IDS[\"tabs\"]).text)) if soup.find(class_=CLASS_IDS[\"tabs\"]) else None\n",
    "    if num_revs_soup:\n",
    "        num_revs_ls = [int(x.replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\")) for x in num_revs_soup]\n",
    "        result_dict[\"total_reviews_num\"] = num_revs_ls[0]\n",
    "        result_dict[\"salaries_posted_num\"] = num_revs_ls[1] if len(num_revs_ls) > 1 else np.nan\n",
    "        result_dict[\"corporate_culture_review_num\"] = num_revs_ls[2] if len(num_revs_ls) > 2 else np.nan\n",
    "    else:\n",
    "        result_dict[\"total_reviews_num\"], result_dict[\"salaries_posted_num\"], result_dict[\"corporate_culture_review_num\"] = np.nan, np.nan, np.nan\n",
    "\n",
    "    ratings_raw = [x.parent.text for x in soup.find_all(class_=re.compile(CLASS_IDS[\"factor_score\"]))]\n",
    "    categories = [rating[3:] for rating in ratings_raw]\n",
    "    ratings = [float(str(rating[:3].replace(\",\", \".\"))) for rating in ratings_raw]\n",
    "    result_dict.update(dict(zip(categories, ratings)))\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def get_applicant_info(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns the review scores and number of reviews by applicants to the company,\n",
    "    separated by the following categories: \"hired\", \"rejected\", \"offerDeclined\", \"deferred\".\n",
    "    \"\"\"\n",
    "    application_outcomes = [\"hired\", \"rejected\", \"offerDeclined\", \"deferred\"]\n",
    "    reviews_by_applicants = {}\n",
    "\n",
    "    for outcome in [\"all_applicants\"] + application_outcomes:\n",
    "        soup = soup_from_url(f\"{url}/bewerbung{'?result=' + outcome if outcome != 'all_applicants' else ''}\")\n",
    "        try:\n",
    "            reviews_by_applicants[f\"{outcome}_review_num\"] = int(soup.find(class_=CLASS_IDS[\"total_reviews\"]).text.split(\" \")[0])\n",
    "            reviews_by_applicants[f\"{outcome}_review_score\"] = float(soup.find(class_=CLASS_IDS[\"aggregation\"]).text[:3].replace(\",\", \".\"))\n",
    "        except:\n",
    "            reviews_by_applicants[f\"{outcome}_review_num\"] = np.nan\n",
    "            reviews_by_applicants[f\"{outcome}_review_score\"] = np.nan\n",
    "\n",
    "    return reviews_by_applicants\n",
    "\n",
    "def get_employee_info(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns the review scores, number of reviews by employees to the company,\n",
    "    and percent of employees that would recommend the company. \n",
    "    \"\"\"\n",
    "    reviews_by_employees = {}\n",
    "    soup = soup_from_url(f\"{url}/kommentare\")\n",
    "\n",
    "    try:\n",
    "        reviews_by_employees[\"employees_review_num\"] = int(soup.find(class_=CLASS_IDS[\"total_reviews\"]).text.split(\" \")[0].replace(\".\", \"\"))\n",
    "        reviews_by_employees[\"employee_review_score\"] = float(soup.find(class_=CLASS_IDS[\"employee_score\"]).text[:3].replace(\",\", \".\"))\n",
    "        reviews_by_employees[\"employee_rec_score\"] = int(soup.find(class_=CLASS_IDS[\"employee_recommendation\"]).text.split(\"%\")[0])\n",
    "    except:\n",
    "        reviews_by_employees[\"employees_review_num\"] = np.nan\n",
    "        reviews_by_employees[\"employee_review_score\"] = np.nan\n",
    "        reviews_by_employees[\"employee_rec_score\"] = np.nan\n",
    "\n",
    "    return reviews_by_employees\n",
    "\n",
    "def get_all_info(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Takes in a url and returns all the information scraped from the URL to ratings_list.\n",
    "    Information collected:\n",
    "    - Firm name\n",
    "    - Firm uuid\n",
    "    - Number of views\n",
    "    - Overall rating\n",
    "    - Percentage of people who would recommend the firm\n",
    "    - Total number of reviews\n",
    "    - Number of salaries posted\n",
    "    - Number of corporate culture reviews\n",
    "    - Ratings for each category\n",
    "    - Number of reviews and scores by applicants\n",
    "    - Number of reviews and scores by employees\n",
    "    \"\"\"\n",
    "    result_dict = main_page_scrape(url)\n",
    "    result_dict.update(get_applicant_info(url))\n",
    "    result_dict.update(get_employee_info(url))\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_mapping = {\n",
    "    'firm_name': 'firm_name',\n",
    "    'url': 'kn_url',\n",
    "    'uuid': 'uuid',\n",
    "    'views_num': 'kn_views_num',\n",
    "    'percent_recommend_overall': 'kn_employee_rec_score',\n",
    "    'overall_rating': 'kn_overall',\n",
    "    'total_reviews_num': 'kn_total_reviews_num',\n",
    "    'salaries_posted_num': 'kn_salaries_posted_num',\n",
    "    'gehaltsozialleistungen': 'kn_salary_benefits',\n",
    "    'image': 'kn_image',\n",
    "    'karriereweiterbildung': 'kn_career_development',\n",
    "    'arbeitsatmosphare': 'kn_work_atmosphere',\n",
    "    'kommunikation': 'kn_communication',\n",
    "    'kollegenzusammenhalt': 'kn_colleague_cohesion',\n",
    "    'work_life_balance': 'kn_work_life_balance',\n",
    "    'vorgesetztenverhalten': 'kn_superiors_behavior',\n",
    "    'interessante aufgaben': 'kn_interesting_tasks',\n",
    "    'arbeitsbedingungen': 'kn_working_conditions',\n",
    "    'umwelt_sozialbewusstsein': 'kn_environment_social_awareness',\n",
    "    'gleichberechtigung': 'kn_equal_rights',\n",
    "    'umgang mit alteren kollegen': 'kn_dealing_with_older_colleagues',\n",
    "    'all_applicants_review_num': 'kn_all_applicants_review_num',\n",
    "    'all_applicants_review_score': 'kn_all_applicants_review_score',\n",
    "    'hired_review_num': 'kn_hired_review_num',\n",
    "    'hired_review_score': 'kn_hired_score',\n",
    "    'rejected_review_num': 'kn_rejected_review_num',\n",
    "    'rejected_review_score': 'kn_rejected_score',\n",
    "    'offerdeclined_review_num': 'kn_offer_declined_review_num',\n",
    "    'offerdeclined_review_score': 'kn_offer_declined_score',\n",
    "    'deferred_review_num': 'kn_deferred_review_num',\n",
    "    'deferred_review_score': 'kn_deferred_score',\n",
    "    'employees_review_num': 'kn_employees_review_num',\n",
    "    'employee_review_score': 'kn_employee_review_score',\n",
    "    'employee_rec_score': 'kn_employee_rec_score',\n",
    "    'corporate_culture_review_num': 'kn_corporate_culture_review_num'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.kununu.com/de/parkhotel-st-leonhard1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kununu_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'firm_name': None,\n",
       " 'url': 'https://www.kununu.com/de/siemens',\n",
       " 'uuid': None,\n",
       " 'views_num': nan,\n",
       " 'percent_recommend_overall': nan,\n",
       " 'overall_rating': nan,\n",
       " 'total_reviews_num': nan,\n",
       " 'salaries_posted_num': nan,\n",
       " 'corporate_culture_review_num': nan,\n",
       " 'all_applicants_review_num': nan,\n",
       " 'all_applicants_review_score': nan,\n",
       " 'hired_review_num': nan,\n",
       " 'hired_review_score': nan,\n",
       " 'rejected_review_num': nan,\n",
       " 'rejected_review_score': nan,\n",
       " 'offerDeclined_review_num': nan,\n",
       " 'offerDeclined_review_score': nan,\n",
       " 'deferred_review_num': nan,\n",
       " 'deferred_review_score': nan,\n",
       " 'employees_review_num': nan,\n",
       " 'employee_review_score': nan,\n",
       " 'employee_rec_score': nan}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_info('https://www.kununu.com/de/siemens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to data/kununu_data_1.csv\n",
      "Saved results to data/kununu_data_2.csv\n",
      "Saved results to data/kununu_data_3.csv\n",
      "Saved results to data/kununu_data_4.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m concurrency \u001b[38;5;241m=\u001b[39m CONCURRENCY\n\u001b[1;32m      5\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(concurrency)\n\u001b[0;32m----> 6\u001b[0m ratings_list \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_all_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_kununu_links\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_kununu_links\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      8\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:634\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 634\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.0/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:334\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Scraping all data in parallel and saving to csv\n",
    "window_size = 5\n",
    "for i in range(0, 10):#len(all_kununu_links)//window_size+1):\n",
    "    concurrency = CONCURRENCY\n",
    "    pool = ThreadPool(concurrency)\n",
    "    ratings_list = pool.map(get_all_info, all_kununu_links[i*window_size:min((i+1)*window_size, len(all_kununu_links))])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    df = pd.DataFrame(ratings_list)\n",
    "    df.columns = [s.replace(\"/\", \"s\").replace(\"-\",\"_\").replace(\"ä\", \"a\").lower() for s in df.columns]\n",
    "    df.rename(columns=column_name_mapping, inplace=True)\n",
    "    df.to_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\", index=False)\n",
    "    print(f\"Saved results to data/kununu_data_{i+1}.csv\")\n",
    "\n",
    "# Consolidate all results\n",
    "df = pd.concat([pd.read_csv(f\"{pwd}/data/kununu_data_{i+1}.csv\") for i in range(0, len(all_kununu_links)//window_size+1)], ignore_index=True)\n",
    "df.to_csv(f\"{pwd}/data/all_scraped_kununu_data.csv\", index=False)\n",
    "print(\"Saved results to data/all_scraped_kununu_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vlh9acpqLAmT"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
