{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Scraping all Written Reviews in Kununu\n",
    "This notebook contains code to scrape text reviews from Kununu's employee review pages.\n",
    "\n",
    "### Workflow\n",
    "1. **Parsing Individual Review Blocks (`parse_review_block`):**\n",
    "   Extracts structured data from a single review, including subcategories and text of reviews.\n",
    "\n",
    "2. **Fetching Reviews (`get_reviews_for_url`):**\n",
    "   Scrapes all reviews for a given company across multiple pages, iterating through the total number of review pages.\n",
    "\n",
    "3. **Batch Scraping (`scrape_all_reviews`):**\n",
    "   Processes reviews for multiple companies in parallel using a thread pool for efficiency. Results are saved periodically to a JSON file.\n",
    "\n",
    "### Output\n",
    "- **Structured Review Data:** A JSON file containing all reviews for the specified companies.\n",
    "\n",
    "### Notes\n",
    "- The scraper relies on specific HTML structures defined in the `CSS_CLASSES` dictionary. Changes to Kununuâ€™s website may require updates to these selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests pandas tqdm python-dotenv beautifulsoup4\n",
    "import pandas as pd\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from utils import *\n",
    "from config import *\n",
    "\n",
    "load_dotenv() # make sure to have a .env file that defines the variable 'SCRAPINGBEE_API_KEY' if using scrapingbee\n",
    "\n",
    "# Set the number of threads to use for concurrent requests\n",
    "CONCURRENCY = 100\n",
    "\n",
    "df = pd.read_csv('data/kununu_data.csv', low_memory=False)[['url', 'employees_review_num']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yIg22RMZpWr"
   },
   "outputs": [],
   "source": [
    "def parse_review_block(block, kn_url):\n",
    "    review = {}\n",
    "    review['kn_url'] = kn_url\n",
    "    # Overall score\n",
    "    score_el = block.select_one(CSS_CLASSES[\"overall_score\"])\n",
    "    review['overall_score'] = float(score_el.text.replace(',', '.')) if score_el else None\n",
    "    \n",
    "    # Title\n",
    "    title_el = block.select_one(CSS_CLASSES[\"title\"])\n",
    "    review['title'] = title_el.text.strip() if title_el else None\n",
    "    \n",
    "    # Date\n",
    "    date_el = block.select_one(CSS_CLASSES[\"date\"])\n",
    "    if date_el:\n",
    "        date_str = date_el.get('datetime', '')\n",
    "        date_parts = date_str.split('T')[0].split('-')\n",
    "        review['year'] = int(date_parts[0])\n",
    "        review['month'] = int(date_parts[1])\n",
    "    else:\n",
    "        review['year'] = None\n",
    "        review['month'] = None\n",
    "    \n",
    "    # Recommendation\n",
    "    rec_el = block.select_one(CSS_CLASSES[\"recommendation_block\"])\n",
    "    if rec_el:\n",
    "        recommendation_text = rec_el.text.strip().lower()\n",
    "    else:\n",
    "        review['recommended'] = None\n",
    "\n",
    "    # Employee Type and Position\n",
    "    emp_info_el = block.select_one(CSS_CLASSES[\"employment_info\"])\n",
    "    if emp_info_el:\n",
    "        emp_info_text = emp_info_el.text.strip()\n",
    "        emp_type_el = emp_info_el.select_one('b')\n",
    "        review['employee_type'] = emp_type_el.text.strip() if emp_type_el else None\n",
    "        try:\n",
    "            position_text = emp_info_el.text.replace(review['employee_type'], '', 1).strip()\n",
    "            review['position'] = position_text if position_text else None\n",
    "        except:\n",
    "            review['position'] = None\n",
    "    else:\n",
    "        review['employee_type'] = None\n",
    "        review['position'] = None\n",
    "\n",
    "    # Subcategories\n",
    "    review['subcategories'] = []\n",
    "    factors = block.select(CSS_CLASSES[\"factor\"])\n",
    "    for f in factors:\n",
    "        cat_title_el = f.select_one(CSS_CLASSES[\"factor_title\"])\n",
    "        if not cat_title_el:\n",
    "            continue\n",
    "        cat_title = cat_title_el.text.strip()\n",
    "        text_el = f.select_one(CSS_CLASSES[\"factor_text\"])\n",
    "        cat_text = text_el.text.strip() if text_el else None\n",
    "        review['subcategories'].append({cat_title: cat_text})\n",
    "\n",
    "    return review\n",
    "\n",
    "def get_reviews_for_url(kn_url, kn_employees_review_num):\n",
    "    reviews = []\n",
    "    total_pages = math.ceil(kn_employees_review_num / 10)\n",
    "    for page_num in range(1, total_pages+1):\n",
    "        url = f\"{kn_url}/kommentare?{page_num}sort=newest\"\n",
    "        soup = soup_from_url(url)\n",
    "        review_blocks = soup.select(CSS_CLASSES[\"review_block\"])\n",
    "        for block in review_blocks:\n",
    "            parsed = parse_review_block(block, kn_url)\n",
    "            if parsed.get('title'):\n",
    "                reviews.append(parsed)\n",
    "    return reviews\n",
    "\n",
    "def scrape_all_reviews(df, save_path=\"data/scraped_reviews.json\"):\n",
    "    results = {}\n",
    "    counter = 0  # Counter to track the number of links scraped\n",
    "    progress_bar = tqdm(total=len(df), desc=\"Scraping reviews\")  # Initialize tqdm\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:\n",
    "        futures = {executor.submit(get_reviews_for_url, row['kn_url'], int(row['kn_employees_review_num'])): row['kn_url'] for _, row in df.iterrows()}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            kn_url = futures[future]\n",
    "            try:\n",
    "                results[kn_url] = future.result()\n",
    "                counter += 1\n",
    "                progress_bar.update(1)  # Update tqdm for each completed task\n",
    "                # Save results every 10,000 links scraped\n",
    "                if counter % 10000 == 0:\n",
    "                    with open(save_path, \"w\") as f:\n",
    "                        json.dump(results, f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {kn_url}: {e}\")\n",
    "    \n",
    "    progress_bar.close()  # Close tqdm after finishing\n",
    "\n",
    "    # Final save at the end\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    print(\"Final data saved.\")\n",
    "    return results\n",
    "\n",
    "scrape_all_reviews(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vlh9acpqLAmT"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
